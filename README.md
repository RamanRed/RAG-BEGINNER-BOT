# RAG-BEGINNER-BOT

ğŸ“Œ Project Overview
The Beginner RAG Bot is an AI-powered system that reads a given story, stores it in a vector database, and answers user questions about the content. It utilizes retrieval-augmented generation (RAG) to fetch relevant information from stored documents and generate accurate responses using a large language model (LLM).

ğŸ› ï¸ Technologies Used
ChromaDB â€“ To store and retrieve document embeddings efficiently.
LangChain â€“ For document loading and text chunking.
Sentence Transformers â€“ To generate vector embeddings for similarity searches.
Google Gemini API â€“ To generate natural language answers from retrieved content.
Python â€“ The core programming language for development.
ğŸ”¹ How It Works
Load Story Documents ğŸ“–

The bot loads markdown (.md) or text files containing the story.
Preprocess & Store Data ğŸ”

The document is split into overlapping chunks using RecursiveCharacterTextSplitter.
Each chunk is converted into a vector embedding using Sentence Transformers.
The embeddings are stored in ChromaDB for efficient retrieval.
Query Processing ğŸ¤–

The user asks a question related to the story.
The bot retrieves the most relevant text chunks from ChromaDB.
The retrieved chunks are passed to the Gemini API, which formulates a coherent answer.
Generate AI-Powered Responses âœ¨

The Gemini API combines retrieved information with LLM reasoning to return an accurate response.
The user receives a contextually relevant answer!
ğŸš€ Features
âœ… Story-based Question Answering â€“ Ask anything about the story, and get an AI-generated response.
âœ… Efficient Retrieval â€“ Uses semantic search for accurate chunk selection.
âœ… Dynamic Content Processing â€“ Updates stored data when a new story is added.
âœ… Fast & Lightweight â€“ Uses ChromaDB for quick vector searches.

ğŸ”® Future Enhancements
ğŸ”¹ Improve chunking strategy for better context retention.
ğŸ”¹ Support multiple document formats like PDF and EPUB.
ğŸ”¹ Add voice-based interaction for better user experience.
ğŸ”¹ Optimize response generation using fine-tuned LLMs.
